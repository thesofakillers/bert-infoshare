#!/bin/bash

#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=DEP_Train
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=00:30:00
#SBATCH --mem=32000M
#SBATCH --array=1-45%2
#SBATCH --output=lisa/outputs/train_logs/dep/xlm_%A_%a.out

module purge
module load 2021
module load Anaconda3/2021.05

source activate bert-infoshare

HPARAMS_FILE=./lisa/arrays/xlm_hparams.txt
srun python -u train.py --num_workers 3 --task DEP \
                        --encoder_name xlm-roberta-base \
                        $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)
